{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Rj7GKuU_qMOSEBWjQPqKWfXOqodZyY_l",
      "authorship_tag": "ABX9TyPfeTaFq5mvW1RiNf8tAmSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1pawn0/persian-speech-to-text-via-whisper/blob/main/whisper_finetune_persian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch_ver = torch.__version__.split('+')[0].split('.')\n",
        "if torch_ver[1] == '8':\n",
        "    print('Installing torchcodec 0.7')\n",
        "    %pip install -q torchcodec==0.7\n",
        "    import torchcodec\n",
        "elif torch_ver[1] == '9':\n",
        "    print('Installing torchcodec 0.8')\n",
        "    %pip install -q torchcodec==0.8\n",
        "    import torchcodec\n",
        "%pip install -q evaluate jiwer\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "TOU7JJJC5GBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc\n",
        "import httpx\n",
        "import torch, torchcodec\n",
        "import polars as pl\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import userdata\n",
        "from transformers import (\n",
        "    BatchFeature,\n",
        "    WhisperConfig,\n",
        "    WhisperTokenizer,\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "import evaluate\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "model_name = \"aictsharif/whisper-base-fa\""
      ],
      "metadata": {
        "id": "r3TCAsbIZ72B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Docs: https://huggingface.co/docs/transformers/main/en/model_doc/whisper\n",
        "\n",
        "https://huggingface.co/blog/fine-tune-whisper#fine-tuning-whisper-in-a-google-colab"
      ],
      "metadata": {
        "id": "J__x-3qs7DiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = WhisperForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
        "fe = WhisperFeatureExtractor.from_pretrained(model_name)\n",
        "tok = WhisperTokenizer.from_pretrained(model_name, language=\"persian\", task=\"transcribe\")\n",
        "model.generation_config.language = \"persian\"\n",
        "model.generation_config.task = \"transcribe\""
      ],
      "metadata": {
        "id": "GVOBKLTSz6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title download and extract the dataset\n",
        "\n",
        "base_url = \"https://datacollective.mozillafoundation.org/api\"\n",
        "api_key = userdata.get(\"MOZILLA_API_KEY\")\n",
        "client_id = userdata.get(\"MOZILLA_CLIENT_ID\")\n",
        "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
        "res: dict = httpx.post(f\"{base_url}/datasets/cmflnuzw5j6l58skwzpc4ze0q/download\", headers=headers).json()\n",
        "filename = res[\"filename\"]\n",
        "download_url = res[\"downloadUrl\"]\n",
        "filesize = res[\"sizeBytes\"]\n",
        "content_type = res[\"contentType\"]\n",
        "expires_at = res[\"expiresAt\"]\n",
        "one_MB = int(2**20)\n",
        "!wget --header=\"Authorization: Bearer {api_key}\" -O \"{filename}\" \"{download_url}\"\n",
        "print(\"Be Patient!\")\n",
        "!tar -xzf \"{filename}\"\n",
        "print(\"The dataset has extracted.\")\n",
        "ds_path = Path('./cv-corpus-23.0-2025-09-05/fa')\n",
        "for fpath in ds_path.iterdir():\n",
        "    print(fpath)"
      ],
      "metadata": {
        "id": "DMTgmfXouIOZ",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run it if you already have the tarfile in your google drive\n",
        "!tar -xzf \"/content/drive/MyDrive/tmp/mcv-scripted-fa-v23.0.tar.gz\" -C \"/content/\""
      ],
      "metadata": {
        "id": "DeklG1lC7XsM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title read tsv files using polars\n",
        "\n",
        "train = pl.read_csv(\n",
        "    source=\"cv-corpus-23.0-2025-09-05/fa/train.tsv\",\n",
        "    separator=\"\\t\",\n",
        "    use_pyarrow=True,\n",
        ")\n",
        "validated = pl.read_csv(\n",
        "    source=\"cv-corpus-23.0-2025-09-05/fa/validated.tsv\",\n",
        "    separator=\"\\t\",\n",
        "    use_pyarrow=True,\n",
        ")\n",
        "validated_sentences = pl.read_csv(\n",
        "    source=\"cv-corpus-23.0-2025-09-05/fa/validated.tsv\",\n",
        "    separator=\"\\t\",\n",
        "    use_pyarrow=True,\n",
        ")\n",
        "\n",
        "other = pl.read_csv(\n",
        "    source=\"cv-corpus-23.0-2025-09-05/fa/other.tsv\",\n",
        "    separator=\"\\t\",\n",
        "    use_pyarrow=True,\n",
        ")\n",
        "dev = pl.read_csv(\n",
        "    source=\"cv-corpus-23.0-2025-09-05/fa/dev.tsv\",\n",
        "    separator=\"\\t\",\n",
        "    use_pyarrow=True,\n",
        ")\n",
        "clip_durations = pl.read_csv(\n",
        "    source=\"cv-corpus-23.0-2025-09-05/fa/clip_durations.tsv\",\n",
        "    separator=\"\\t\",\n",
        "    use_pyarrow=True,\n",
        ")\n",
        "\n",
        "# max_chars = train[\"sentence\"].str.len_chars().quantile(.99) # = 63\n",
        "train = train.filter(pl.col(\"sentence\").str.len_chars() < 64).with_row_index()\n",
        "\n",
        "train = (\n",
        "    train.join(clip_durations, left_on=\"path\", right_on=\"clip\", how=\"left\")\n",
        "    .with_columns(pl.col(\"duration[ms]\").truediv(1000).alias(\"duration\"))\n",
        "    .drop(\"duration[ms]\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "I8qXFmSJO9aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title find tokenized input_ids' max_length\n",
        "# # Tokenize all of the input sentences to find the highest number of input_ids length\n",
        "# target_labels = tok.__call__(\n",
        "#     train[\"sentence\"].to_list(),\n",
        "#     return_tensors=\"pt\",\n",
        "#     return_attention_mask=False,\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "# ).input_ids.squeeze(0)\n",
        "\n",
        "# print(target_labels.shape) # torch.Size([29547, 52]), therefore the max_length is `52`\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0FVSrzg6QjP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pl.DataFrame,\n",
        "        feature_extractor: WhisperFeatureExtractor,\n",
        "        tokenizer: WhisperTokenizer,\n",
        "        audio_files_dir: Path,\n",
        "        target_sample_rate: int = 16000,\n",
        "        max_duration: int = 30,\n",
        "    ):\n",
        "        self.df = df\n",
        "        self.fe = feature_extractor\n",
        "        self.tok = tokenizer\n",
        "        self.fdir = audio_files_dir\n",
        "        self.sr = target_sample_rate\n",
        "        self.duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform: torch.Tensor = (\n",
        "            torchcodec.decoders.AudioDecoder(\n",
        "                source=self.fdir / self.df[\"path\"][idx],\n",
        "                sample_rate=self.sr,\n",
        "                num_channels=1,\n",
        "            )\n",
        "            .get_all_samples()\n",
        "            .data.squeeze(0)[: self.sr * self.duration]\n",
        "        )\n",
        "\n",
        "        features: BatchFeature = self.fe.__call__(\n",
        "            raw_speech=waveform,\n",
        "            return_tensors=\"pt\",\n",
        "            sampling_rate=self.sr,\n",
        "            do_normalize=True,\n",
        "            return_attention_mask=False,\n",
        "        ).input_features.squeeze(0)\n",
        "\n",
        "        labels: torch.Tensor = self.tok.__call__(\n",
        "            self.df[\"sentence\"][idx],\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=False,\n",
        "            # precalculated_longest_input_ids_tensor_length = 52\n",
        "            max_length=52,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        ).input_ids.squeeze(0)\n",
        "\n",
        "        return {'input_features': features, \"labels\": labels}\n",
        "\n",
        "\n",
        "audio_files_path = Path(\"./cv-corpus-23.0-2025-09-05/fa/clips\")\n",
        "ds = AudioDataset(train, fe, tok, audio_files_path)\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_size = int(0.9 * len(ds))\n",
        "eval_size = len(ds) - train_size\n",
        "train_ds, eval_ds = random_split(ds, [train_size, eval_size], generator=generator1)"
      ],
      "metadata": {
        "id": "I5eeNT5X2ydg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tok.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tok.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tok.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n"
      ],
      "metadata": {
        "id": "OHfS6tsTs9c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-base-fa\",\n",
        "    num_train_epochs=1,\n",
        "    auto_find_batch_size=True,\n",
        "    gradient_checkpointing=False,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=1e-6,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"best\",\n",
        "    predict_with_generate=True,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    logging_first_step=True,\n",
        "    dataloader_pin_memory=True,\n",
        "    greater_is_better=False,\n",
        "    disable_tqdm=False,\n",
        "    torch_compile=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "\n",
        ")\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=fe,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "nOHOG3PrvDhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FyI1NtZBGQby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}