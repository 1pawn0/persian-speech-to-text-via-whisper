{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1EoSlRKD8Y8Ux1Dc8iTtOz5cBmzjpUyec",
      "authorship_tag": "ABX9TyO0bPc0lrK/bTDCqHNRioQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1pawn0/persian-speech-to-text-via-whisper/blob/main/persian_speech_to_text_via_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install torchcodec"
      ],
      "metadata": {
        "id": "cMcORdkN4llu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKJjfXXwz00V"
      },
      "outputs": [],
      "source": [
        "import torch, torchaudio, torchcodec\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    BatchFeature,\n",
        "    WhisperConfig,\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperForConditionalGeneration,\n",
        ")\n",
        "\n",
        "device = \"cuda\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"nezamisafa/whisper-v3-turbo-persian-v1.0\"  # @param {\"type\":\"string\",\"placeholder\":\"openai/whisper-large-v3-turbo\"}\n",
        "model_config = WhisperConfig.from_pretrained(MODEL_NAME)\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME, config=model_config).to(device)\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "EyIRw63Z0EP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_audio_into_chunks(audio_file_path: Path, chunk_duration_sec: int = 30, target_sample_rate: int = 16000) -> list[torch.Tensor]:\n",
        "    waveform, orig_sample_rate = torchaudio.load_with_torchcodec(audio_file_path)\n",
        "\n",
        "    if orig_sample_rate != target_sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=target_sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    chunk_samples = chunk_duration_sec * target_sample_rate\n",
        "    total_samples = waveform.shape[1]\n",
        "\n",
        "    chunks: list[torch.Tensor] = []\n",
        "    for start in range(0, total_samples, chunk_samples):\n",
        "        end = start + chunk_samples\n",
        "        chunk = waveform[0, start:end]\n",
        "\n",
        "        if chunk.shape[0] < chunk_samples:\n",
        "            padding = chunk_samples - chunk.shape[0]\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, padding), mode=\"constant\", value=0.0)\n",
        "\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "I4_Vqa202SJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_transcription(audio_chunks: list[torch.Tensor], generated_file_path: Path):\n",
        "    f = open(generated_file_path, \"w\", encoding=\"utf-8\")\n",
        "    for chunk in audio_chunks:\n",
        "        features: BatchFeature = feature_extractor(\n",
        "            raw_speech=chunk,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            do_normalize=True,\n",
        "            return_attention_mask=True,\n",
        "            device=device,\n",
        "        ).to(device)\n",
        "\n",
        "        pred_ids: torch.Tensor = model.generate(\n",
        "            input_features=features.input_features.to(device),\n",
        "            attention_mask=features.attention_mask.to(device),\n",
        "            language=\"fa\",\n",
        "            task=\"transcribe\",\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        text = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
        "        f.write(text)\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "FtXT8HHp42e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_speech(audio_file_path: Path) -> Path:\n",
        "    audio_chunks: list[torch.Tensor] = split_audio_into_chunks(audio_file_path)\n",
        "\n",
        "    transcription_file_path: Path = Path(f\"{audio_file_path.stem}.txt\")\n",
        "    generate_transcription(audio_chunks, transcription_file_path)\n",
        "    return transcription_file_path\n",
        "\n",
        "\n",
        "speech_audio_file_path: str = \"path/to/speech/audio.mp3\"  # @param {\"type\":\"string\",\"placeholder\":\"path/to/speech/audio.mp3\"}\n",
        "transcribe_speech(Path(speech_audio_file_path))\n"
      ],
      "metadata": {
        "id": "C5fPTgEDZI0k",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}